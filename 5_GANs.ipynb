{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversial Networks (GANs)\n",
    "\n",
    "As the name suggests these are generative models (can produce novel samples) that estimate/model the density of distribution implicitly i,e. no loss function in terms of probabilistic density."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Basic Idea\n",
    "\n",
    "Compare the real and synthetic images to a point where the synthetic images look real. The Generator clashes with the Discriminator, hence the word adverserial is used.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./doc_imgs/GAN_architecture.png\" width=\"\" height=\"300\" />\n",
    "</p>\n",
    "\n",
    "* The generator tries to generate novel (real looking) samples from noise\n",
    "* The Disc. distuinguishes between real and fake images and forces the Gen. to produce successively more realistic samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "**Discriminator Loss:**\n",
    "\n",
    "$$\n",
    "J^{(D)} = -(1/2)E_{x\\sim p_{data}}logD(x) - (1/2)E_{z \\sim p_{z}(z)}log(1-D(G(z)))\\\\\n",
    "where \\ J^{(D)}\\ is\\ simply\\ binary\\ cross\\ entropy\\ loss\n",
    "$$\n",
    "\n",
    "**Generator Loss:**\n",
    "\n",
    "$$\n",
    "J^{(G)} = -J^{(D)}\\\\\n",
    "D \\ provides \\ supervision \\ for\\ the\\ gradient \\ of G\\\\\n",
    "we\\ can\\ also\\ say\\ that\\ D\\ is\\ the\\ learnable\\ loss\\ fn\\\\ \n",
    "\n",
    "\\\\[0.2in]\n",
    "\\\\ \\textrm{in  practice the following loss function is used for the generator: }\\\\\n",
    "J^{(G)} = - (1/2)E_{z}log(D(G(z)))\n",
    "$$\n",
    "\n",
    "**Minimax Game:**\n",
    "\n",
    "D tries to maximize the probability it correctly classifies reals and fakes and G tries to minimize the probability that D will predict its outputs are fake (notice the 1-x and x terms in the loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Essential Imports\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "\n",
    "\n",
    "import einops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# The Generator\n",
    "class G(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.LazyLinear(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.LazyLinear(256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "\n",
    "            nn.LazyLinear(512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.LazyLinear(1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.LazyLinear(784),\n",
    "            # nn.Tanh()\n",
    "            nn.ReLU()\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.upsample(x)\n",
    "        x = einops.rearrange(x, 'n (c h w) -> n c h w', h = 28, w = 28)\n",
    "        return x\n",
    "\n",
    "# The Discriminator\n",
    "class D(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            \n",
    "            nn.LazyLinear(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LazyLinear(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.LazyLinear(1),\n",
    "            nn.Sigmoid()\n",
    "        \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = einops.rearrange(x, 'n c h w -> n (c h w)') #flatten imgs\n",
    "        return self.downsample(x)\n",
    "\n",
    "\n",
    "# The GAN class\n",
    "\n",
    "class GAN(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "        latent_dim = 32,\n",
    "        lr = 0.0002 # as described in DCGAN Paper\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.lr = lr\n",
    "        self.gen = G()\n",
    "        self.disc = D()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.val_z = torch.randn(8, self.latent_dim) # Fix this for deteministic outputs\n",
    "\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                # transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Have to implement this method because of pl\n",
    "        return self.gen(z)\n",
    "\n",
    "    def bce_loss(self, y_pred, y):\n",
    "        x = F.binary_cross_entropy(y_pred, y)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "\n",
    "        opt_g = torch.optim.Adam(self.gen.parameters(), lr= self.lr, betas= (0.5, 0.99))  # as described in DCGAN Paper\n",
    "        opt_d = torch.optim.Adam(self.disc.parameters(), lr= self.lr, betas= (0.5, 0.99)) # as described in DCGAN Paper\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs, _ = batch\n",
    "\n",
    "        # we sample a new z from a normal dist at each training step\n",
    "        z = torch.randn(imgs.size(0), self.latent_dim).cuda()\n",
    "        \n",
    "        # Generator loss:  \n",
    "        # Recall: J(G) = - (1/2)log(D(G(z)))\n",
    "        if optimizer_idx == 0:\n",
    "            fake_imgs = self(z)\n",
    "            D_G_z = self.disc(fake_imgs)\n",
    "            labels = torch.ones(fake_imgs.size(0), 1).cuda()\n",
    "            g_loss = self.bce_loss(D_G_z, labels)\n",
    "            self.log(\"g_loss\", g_loss, prog_bar=True)\n",
    "            return g_loss\n",
    "            \n",
    "\n",
    "        # Disc loss\n",
    "        # Recall: J^{(D)} = -(1/2)E_{x\\sim p_{data}}logD(x) - (1/2)E_{z \\sim p_{z}(z)}log(1-D(G(z)))\\\\\n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            labels = torch.ones(imgs.size(0), 1).cuda()\n",
    "            real_loss = 0.5*(self.bce_loss(self.disc(imgs), labels))\n",
    "\n",
    "            labels = torch.zeros(imgs.size(0), 1).cuda()\n",
    "            fake_loss = 0.5*(self.bce_loss(self.disc(self.gen(z).detach()), labels))\n",
    "\n",
    "            d_loss = real_loss + fake_loss\n",
    "\n",
    "            self.log(\"d_loss\", d_loss, prog_bar=True)\n",
    "            return d_loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "            z = self.val_z.cuda()\n",
    "\n",
    "            # log sampled images\n",
    "            sample_imgs = self(z)\n",
    "            grid = torchvision.utils.make_grid(sample_imgs.detach().cpu(), nrow=2, normalize=True, range=(-1, 1))\n",
    "            # plt.imshow(sample_imgs[0][0].detach().cpu().numpy())\n",
    "            # plt.show()\n",
    "            self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        dataset = datasets.MNIST('/home/ibrahim/Projects/Datasets/', train= True, download= True, transform= self.transform)\n",
    "        train_loader  = DataLoader(dataset=dataset, batch_size= 32, shuffle= True, drop_last= True, num_workers= 12)\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = datasets.MNIST('/home/ibrahim/Projects/Datasets/', train= False, download= True, transform= self.transform)\n",
    "        val_loader  = DataLoader(dataset=val_dataset, batch_size= 16, shuffle= True, drop_last= True, num_workers= 12)\n",
    "\n",
    "        return val_loader\n",
    "\n",
    "\n",
    "model = GAN()\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    max_epochs=100,\n",
    "    callbacks=[TQDMProgressBar(refresh_rate=20)],\n",
    "    enable_checkpointing= False,\n",
    "    logger= True\n",
    ")\n",
    "\n",
    "trainer.fit(model)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some GAN Hacks (take with a grain of salt)**\n",
    "* Normalize inputs between -1 and 1\n",
    "* use Tanh as the last layer of the generator output (because of above)\n",
    "* Sample $z$ from a Gaussian dist\n",
    "* SGD for $D$\n",
    "* ADAM for $G$\n",
    "* Use BatchNorm between layers\n",
    "* One sided label smoothing: \n",
    "$$\n",
    "J^{(D)} = -(1/2)(\\lambda)E_{x\\sim p_{data}}logD(x) - (1/2)E_{z}log(1-D(G(z)))\\\\\n",
    "where\\ \\lambda \\ is\\ a\\ small\\ value\\ <\\ 1\\ e.g\\ 0.9  \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov  4 2022, 20:59:55) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3a77b3f6b54150433cb32c6ba4a5906806200d1f5473f6dbf041d5a5759c3200"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
